# SonoVLM: A Generalist Ultrasound Vision-Language Model  


A multimodal AI system for ultrasound analysis with capabilities in cross-organ understanding, abnormality detection, diagnostic reasoning, structured reporting, and patient-centric dialogue.

---

## ğŸ”¥ Latest News  
- **2025/05/24**: ğŸ‰ Official repository launched!  
- **2025/06/10**: Code is now available
> ğŸ“Œ **Code is now publicly available** 

---

## ğŸ§  Key Features  
- **Multi-Organ Analysis**: 7 anatomical structures supported (e.g., thyroid, breast, liver, heart, kidney)  
- **Disease Diagnosis**: Ultrasound-based classification
- **Diagnostic Reasoning**:  disease differentiation 
- **Ultrasound Reporting**: Automatic structured report generation 
- **Clinical Dialogue**: Multi-turn conversation support for doctor-patient-system interaction

---
## ğŸ”¥ Demo  
<video controls>
  <source src="VLMæ¼”ç¤º.mp4" type="video/mp4">
  Your browser does not support the video tag.
</video>
## ğŸ“ˆ Technical Architecture  
pass

## ğŸš€ Getting Started  
**Note**: Code will be released publicly after publication of our paper.  
<details>
<summary><b>1. Installation (Coming Soon) </b></summary>
pass
</details>
<details>
<summary><b>2. Prepare your finetuning data</b></summary>

Like LLaVA, we anticipate that the data will reside within a JSON file, composed of a collection of dictionaries. In this structure, each individual dictionary corresponds to a distinct sample.
```json
   [
    {
        "id": "215168",
        "system_prompt": "You are a helpful assistant.",
        "image": [
            "215168_1.jpeg",
            "215168_2.jpeg"
        ],
        "description": "åŒä¾§ä¹³è…ºè…ºä½“ç»“æ„ç¨ç´Šä¹±ï¼Œä¹³å¯¼ç®¡ä¸æ‰©å¼ ï¼ŒåŒä¾§ä¹³è…ºæœªè§æ˜ç¡®å ä½æ€§ç—…å˜ã€‚åŒä¾§è…‹ä¸‹æœªè§æ˜æ˜¾è‚¿å¤§æ·‹å·´ç»“ã€‚",
        "conversations": [
            {
                "from": "human",
                "value": "<image><image>Based on the ultrasound image, can you speculate on the functional status of the examined organ?"
            },
            {
                "from": "gpt",
                "value": "åŒä¾§ä¹³è…ºè…ºä½“ç»“æ„ç¨ç´Šä¹±ï¼Œä¹³å¯¼ç®¡ä¸æ‰©å¼ ï¼ŒåŒä¾§ä¹³è…ºæœªè§æ˜ç¡®å ä½æ€§ç—…å˜ã€‚åŒä¾§è…‹ä¸‹æœªè§æ˜æ˜¾è‚¿å¤§æ·‹å·´ç»“ã€‚"
            },
        ]
    }
]
```
</details>
<details>
<summary><b>3. Perform finetuning</b></summary>
   
```
deepspeed train.py 
```
You can modify the parameter settings as needed, such as 
   ```
   deepspeed train.py
--per_device_train_batch_size 16
```
